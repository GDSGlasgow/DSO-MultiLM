{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "from xml.etree import ElementTree as ET\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from gpt_geoparser.geoparser import GptGeoparser\n",
    "from gpt_geoparser.gpt_handler import PromptBuilder\n",
    "from gpt_geoparser.data import GeoVirusArticle, LGLArticle, Article, Toponym, News2024Article, WikTorArticle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fine-tuning dataset for Llama-3 Geoparser fine tuning\n",
    "We will construct a dataset using the LgL and GeoVirus datasets with which we will fine-tune a custom Llama-3 model. The model will be tested on the News2024 dataset to assess the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "geollama_prompt =\"\"\"Below is an instruction that describes a task, paired with an input that provides a specfic example which the task should be applied to. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "geoparse_instruction = \"\"\"Extract all toponyms from the provided text and estimate their geolocations. Include the name of every toponym in the text and its decimal latitude and longitude coordinates. Format the output in JSON, strictly adhering to the specified template. Be very concise and output only the JSON data inside a code block. Do not provide any explanation or reasoning.\n",
    "\n",
    "JSON Template for output:\n",
    "\n",
    "{\"toponyms\": [\n",
    "        {\n",
    "          \"name\": \"<string : toponym name exactly as it appears in the text>\",\n",
    "          \"latitude\": <float : latitude in decimal degrees>,\n",
    "          \"longitude\": <float : longitude in decimal degrees>\n",
    "        },\n",
    "        // More toponyms from the text can follow\n",
    "      ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LGL data\n",
    "# open the lgl dataset using xml\n",
    "dataset = 'lgl'\n",
    "\n",
    "def get_data(dataset):\n",
    "    xml = ET.parse(f\"datasets/{dataset}.xml\")\n",
    "    xml_root = xml.getroot()\n",
    "\n",
    "    xml_str = ET.tostring(xml_root,method='xml').decode()\n",
    "    xml_obj = objectify.fromstring(xml_str)\n",
    "    return xml_obj\n",
    "\n",
    "def build_ft_data(xml_obj, dataset):\n",
    "    \n",
    "    ft_data = []\n",
    "    if dataset in ['lgl', 'GeoVirus']:\n",
    "        articles = xml_obj.article\n",
    "    elif dataset in ['WikToR']:\n",
    "        articles = xml_obj.page\n",
    "    for article_xml in articles:\n",
    "        if dataset=='lgl':\n",
    "            article = LGLArticle(article_xml)\n",
    "        elif dataset=='GeoVirus':\n",
    "            article = GeoVirusArticle(article_xml)\n",
    "        elif dataset=='WikToR':\n",
    "            article = WikTorArticle(article_xml)\n",
    "        text = article.text\n",
    "        response = {\"toponyms\":[]}\n",
    "        for toponym in article.toponyms:\n",
    "            try:\n",
    "                response['toponyms'].append({\"name\":str(toponym.phrase),\n",
    "                                            \"latitude\":float(toponym.latitude),\n",
    "                                            \"longitude\":float(toponym.longitude)})\n",
    "            except:\n",
    "                response['toponyms'].append({\"name\":str(toponym.phrase),\n",
    "                                             \"latitude\":None,\n",
    "                                             \"longitude\":None})\n",
    "        ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                        \"input\":str(text),\n",
    "                        \"response\":response})\n",
    "    \n",
    "    return ft_data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgl_xml = get_data('lgl')\n",
    "geovirus_xml = get_data('GeoVirus')\n",
    "\n",
    "lgl_ft_data = build_ft_data(lgl_xml, 'lgl')\n",
    "geovirus_ft_data = build_ft_data(geovirus_xml, 'GeoVirus')\n",
    "\n",
    "ft_data = lgl_ft_data + geovirus_ft_data\n",
    "\n",
    "with open('datasets/fine_tuning/llama3_ft_data.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_data('TR-News')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fine-tuning dataset for RAG based Llama-3 Geoparser\n",
    "We will construct a dataset using the LgL and GeoVirus datasets with which we will fine-tune a custom Llama-3 model. The model will be tested on the News2024 dataset to assess the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgl_xml = get_data('lgl')\n",
    "geovirus_xml = get_data('GeoVirus')\n",
    "\n",
    "ft_articles = []\n",
    "for article_xml in lgl_xml.article:\n",
    "    ft_articles.append(LGLArticle(article_xml))\n",
    "for article_xml in geovirus_xml.article:\n",
    "    ft_articles.append(GeoVirusArticle(article_xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides a specfic example which the task should be applied to. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparse_instruction = \"\"\"You will be given a piece of text, a toponym found within that text, and a JSON detailing the matched locations when that toponym is searched on OpenStreetMaps. \n",
    "\n",
    "Your task is to identify the matched location which is most likely to be the true location of the toponym, given the context of the text.\n",
    "\n",
    "If the list of matches is empty, or you do not think any match accurately represents the toponym, you are permitted to assign your best estimate for a latitude and longitude. This should be highlighted in your response by setting {\"RAG\":false}.\n",
    "\n",
    "Your output should strictly conform to the following tmeplate:\n",
    "\n",
    "{\"name\" : <(str) name of toponym as it appears in the text>,\n",
    " \"latitude\": <(float) latitude as it appears in the matched locations>,\n",
    " \"longitude\": <(float) longitude as it appears in the matched locations>,\n",
    " \"RAG_estimated\": <(bool) true if a matched location was used>\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = r\"\"\"<text> {} <\\text>\n",
    "\n",
    "<toponym> {} <\\toponym>\n",
    "\n",
    "<matches> {} <\\matches>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nominatim_cache.json', 'r') as f:\n",
    "    cache = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [00:07<00:00, 116.59it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_data = []\n",
    "\n",
    "for article in tqdm(ft_articles):\n",
    "    \n",
    "    for toponym in article.toponyms:\n",
    "        if not toponym.latitude:\n",
    "            continue\n",
    "        true_point = (float(toponym.latitude), float(toponym.longitude))\n",
    "        matches = cache[toponym.phrase]\n",
    "        best_match = None\n",
    "        best_d = np.inf\n",
    "        for match in matches:\n",
    "            match_point = (float(match['lat']), float(match['lon']))\n",
    "            d = distance.distance(match_point, true_point)\n",
    "            if d < best_d:\n",
    "                best_match = match\n",
    "                best_d = d\n",
    "        # check if any match was very good:\n",
    "        if len(matches)==0:\n",
    "            response = {'name':toponym.phrase,\n",
    "                        'latitude':toponym.latitude,\n",
    "                        'longitude':toponym.longitude,\n",
    "                        'RAG_estimated':False}\n",
    "            \n",
    "        elif (best_d.km > 20) and (best_match['addresstype'] not in ['country', 'state', 'county', 'region']):\n",
    "            response = {'name':toponym.phrase,\n",
    "                        'latitude':toponym.latitude,\n",
    "                        'longitude':toponym.longitude,\n",
    "                        'RAG_estimated':False}\n",
    "        else:\n",
    "            response = {'name':toponym.phrase,\n",
    "                        'latitude':best_match['lat'],\n",
    "                        'longitude':best_match['lon'],\n",
    "                        'RAG_estimated':True}\n",
    "    \n",
    "        match_info = [{'name':m['name'], 'lat':m['lat'], 'lon':m['lon'], 'address':m['display_name']} for m in matches]\n",
    "        input = input_prompt.format(article.text, toponym.phrase, match_info)\n",
    "        \n",
    "        ft_prompt = RAG_prompt.format(geoparse_instruction, input, response)\n",
    "        ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                        \"input\":input,\n",
    "                        \"response\":str(response)})\n",
    "               \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3983"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deduplicate\n",
    "ft_data = [dict(t) for t in {tuple(d.items()) for d in ft_data}]\n",
    "\n",
    "len(ft_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama3_RAG_geoparsing_ft.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fine-tuning dataset for Llama-3 toponym extraction\n",
    "We will construct a dataset using the LgL and GeoVirus datasets with which we will fine-tune a custom Llama-3 model. The model will be tested on the News2024 dataset to assess the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoparse_instruction = \"\"\"You will be given a piece of text which contains some toponyms. Please extract each toponyhm from the text and place it in a python list.\n",
    "\n",
    "Each toponym should only appear once in the list, even if they occur multiple times in the text. If multiple spellings of the same toponym appear in the text each spelling should be represented in the list.\n",
    "\n",
    "Please use the following template to structure your response:\n",
    "\n",
    "{\"toponyms\":[\"toponym_1\", \"toponym_2\", \"toponym_3\",...]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [00:00<00:00, 45814.57it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_data = []\n",
    "\n",
    "for article in tqdm(ft_articles):\n",
    "    \n",
    "    toponyms = [str(t.phrase) for t in article.toponyms]\n",
    "    response = {\"toponyms\":list(set(toponyms))}\n",
    "    input = article.text\n",
    "    ft_data.append({\"instruction\":geoparse_instruction,\n",
    "                    \"input\":input,\n",
    "                    \"response\":str(response)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama3_toponym_extraction_ft.json', 'w') as f:\n",
    "    json.dump(ft_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'YYou will be given a piece of text which contains some toponyms. Please extract each toponyhm from the text and place it in a python list.\\n\\nEach toponym should only appear once in the list, even if they occur multiple times in the text. If multiple spellings of the same toponym appear in the text each spelling should be represented in the list.\\n\\nPlease use the following template to structure your response:\\n\\n{\"toponyms\":[\"toponym_1\", \"toponym_2\", \"toponym_3\",...]}\\n',\n",
       " 'input': 'Drivers warned to watch out for flooded roads. - Highway 200 east of Mahnomen from Mahnomen County Road 3 to Mahnomen County Road 122 near Twin Lakes - Highway 10 eastbound and westbound east of Perham near Otter Tail County Road 80 - Highway 108 east of Pelican Rapids from Highway 59 to Star Lake - Highway 59 from Otter Tail/Grant county line to north junction Highway 55 - Highway 55 from Grant/Wilkin county line to County Road 43 - Highway 114 at Douglas County Road 4 at the south end of Lake Mary - Highway 9 two blocks east of Highway 12 in Benson These highways are currently open but rapidly changing conditions could call for closures in some areas. Motorists should use extreme caution watch for water on all area roadways. Highway 75 at the Kent underpass remains closed. Motorists should follow the signed detour. The Minnesota Department of Transportation and the Minnesota State Patrol urges motorists to drive with caution as flooding continues to affect area highways. Water over the roadway is currently affecting the following areas in Becker, Clay, Douglas, Grant, Mahnomen, Otter Tail, Swift and Wilkin counties:',\n",
       " 'response': \"{'toponyms': ['Mahnomen County Road', 'Highway 108', 'Otter Tail', 'Swift', 'Highway 114', 'Douglas', 'Douglas County', 'Grant', 'Highway 200', 'Mahnomen', 'County Road 43', 'Clay', 'Highway 10', 'Twin Lakes', 'Otter Tail County', 'Otter Tail/Grant', 'Becker', 'Lake Mary', 'Wilkin', 'Grant/Wilkin', 'Minnesota', 'Perham', 'Benson', 'Star Lake', 'Kent', 'Pelican Rapids']}\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-geoparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
