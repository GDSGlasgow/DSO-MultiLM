{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# thrid party imports\n",
    "import sys\n",
    "sys.path.append('../BERT_geoparser/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "# local imports\n",
    "from train_model import Trainer\n",
    "from tokenizer import Tokenizer\n",
    "from data import Data\n",
    "from model import BertModel\n",
    "from analysis import Results\n"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
>>>>>>> a77f96d0820c52c0cbe32d1d300a3d607231739f
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-tuning a BERT language model on NER data\n",
    "In this notebook we use the `BERT_geoparser` package to build and fine tune a BERT model to perform Named Entity Recognition (NER) tasks. This is the first step in a multi-step process to build and train a BERT model to identify target and incidental locations within text. \n",
    "\n",
    "We use an NER dataset labelled using the B-I-O format, with 8 categories of word - location (`geo`), time (`tim`), organization (`org`), person (`per`), geo-political entity (`gpe`), art/culture (`art`), event (`eve`) or nature (`nat`). Each tag can indicate whether a word is the *begining* of a related phrase (`B`) or *inside* a phrase (`I`). Words which do not belong to any category are given the *outer* tag (`O`). Specialtokens indicating the start (`CLS`) and end (`SEP`) of a sentence are also added. For example, the phrase:\n",
    "\n",
    "<p style=\"text-align: center;\"><span style=\"color:red\">Jane</span> visited <span style=\"color:green\">Madisson Square Gardens</span> while in <span style=\"color:yellow\">New York</span>.</p>\n",
    "\n",
    "Would receive the tags:\n",
    "\n",
    "<p style=\"text-align: center;\"> [CLS] <span style=\"color:red\"> [B-PER] </span> [O] <span style=\"color:green\">[B-ORG] [I-ORG] [I-ORG]</span> [O] [O] <span style=\"color:yellow\">[B-GEO] [I-GEO]</span> [SEP] </p>\n",
    "\n",
    "The Fine tuned bert model can then estimate the most likely sequence of tags for a given sentence, and can provide the confidence on the given tags.\n",
    "\n",
    "## 1.1 Initial training on the CoNLL dataset\n",
    "We will initially train the model on CoNLL-2003 dataset, before retraining the same model on the WikiNeural dataset. This will provide the model with a large number of trainingexamples, while ensuring it is optimized to handel wikipedia style data. This can also be done by running the script `train_model_conll2003.sh` in a Linux Torque environment.\n",
    "\n",
    "Running this cell will train and test the model. The precision, recall and F1 score in each category are given in the `CoNLL_test_results.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data_path = r'../data/NB1/train_CoNLL_dataset.csv', \n",
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "c:\\Users\\jws10y\\AppData\\Local\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building input data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38366/38366 [1:34:34<00:00,  6.76it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'r../models/TopoBERT_CoNLL_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/NB1/CoNLL_train_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m                   model_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                   cased \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      4\u001b[0m                   learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-6\u001b[39m,\n\u001b[0;32m      5\u001b[0m                   max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,\n\u001b[0;32m      6\u001b[0m                   saved_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr../models/TopoBERT_CoNLL.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m              \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(test_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/NB1/CoNLL_test_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m              results_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/CoNLL_test_results.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jws10y\\git\\DSO\\AGILE paper\\notebooks\\../BERT_geoparser\\train_model.py:76\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, save_as, n_epochs, batch_size, val_split)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, save_as, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, val_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m     75\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_weights()\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_as\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jws10y\\git\\DSO\\AGILE paper\\notebooks\\../BERT_geoparser\\model.py:218\u001b[0m, in \u001b[0;36mBertModel.train\u001b[1;34m(self, n_epochs, verbose, batch_size, validation_split, save_as, class_weights)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_as:\n\u001b[0;32m    213\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m [ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39msave_as,\n\u001b[0;32m    214\u001b[0m                   save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m                   monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    216\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    217\u001b[0m                   save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_as\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# get sample weights\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weights:\n",
      "File \u001b[1;32mc:\\Users\\jws10y\\git\\DSO\\AGILE paper\\notebooks\\../BERT_geoparser\\model.py:235\u001b[0m, in \u001b[0;36mBertModel.save_config\u001b[1;34m(self, save_as)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_config\u001b[39m(\u001b[38;5;28mself\u001b[39m, save_as):\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_as\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_config.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m    236\u001b[0m         config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag_dict\u001b[39m\u001b[38;5;124m'\u001b[39m:{k:\u001b[38;5;28mint\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_dict\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    237\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtokenizer_obj\u001b[38;5;241m.\u001b[39msize,\n\u001b[0;32m    238\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcased\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtokenizer_obj\u001b[38;5;241m.\u001b[39mcased,\n\u001b[0;32m    239\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdata_path)}\n\u001b[0;32m    241\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(config, out, default\u001b[38;5;241m=\u001b[39mconvert)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'r../models/TopoBERT_CoNLL_config.json'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(data_path = r'../data/NB1/CoNLL_train_dataset.csv', \n",
>>>>>>> a77f96d0820c52c0cbe32d1d300a3d607231739f
    "                  model_size = 'large',\n",
    "                  cased = True, \n",
    "                  learning_rate=2e-6,\n",
    "                  max_len=80,\n",
    "                  saved_model=False)\n",
    "\n",
    "trainer.train(save_as='r../models/TopoBERT_CoNLL.hdf5',\n",
    "              n_epochs=20,\n",
    "              batch_size=4,\n",
    "              val_split=0.1) \n",
    "              \n",
<<<<<<< HEAD
    "trainer.test(test_data=r'../data/NB1/test_CoNLL_dataset.csv',\n",
    "             results_filename=r'../results/NB1/CoNLL_test_results.txt')"
=======
    "trainer.test(test_data=r'../data/NB1/CoNLL_test_dataset.csv',\n",
    "             results_filename=r'../results/CoNLL_test_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "\n",
    "with open(r'../models/test_config.json', 'w') as out:\n",
    "    config = {'tag_dict':None,\n",
    "            'size': None,\n",
    "            'cased': None,\n",
    "            'data_path':None}\n",
    "    json.dump(config, out, default=\"convert\")"
>>>>>>> a77f96d0820c52c0cbe32d1d300a3d607231739f
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 Retraining on the WikiNeural dataset\n",
    "We can now retrain the saved model on Wikipedia data, using the WikiNeural dataset. Once again, this can be run using `train_model_wikineural.sh`. Test results are given in `wikineural_test_results.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_path = 'wikineural_train_dataset.csv', \n",
    "                  model_size = 'Large',\n",
    "                  cased = True, \n",
    "                  learning_rate=2e-6,\n",
    "                  max_len=80,\n",
    "                  saved_model=r'../models/TopoBERT.hdf5')\n",
    "\n",
    "trainer.train(save_as='../models/TopoBERT_WikiNeural.hdf5',\n",
    "              n_epochs=5,\n",
    "              batch_size=2,\n",
    "              val_split=0.1) \n",
    "              \n",
    "trainer.test(test_data=r'../data/NB1/wikineural_test_dataset.csv',\n",
    "             results_filename=r'../results/NB1/wikineural_test_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
